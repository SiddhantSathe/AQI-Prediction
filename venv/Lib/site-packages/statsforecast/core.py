# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/src/core/core.ipynb.

# %% auto 0
__all__ = ['StatsForecast']

# %% ../nbs/src/core/core.ipynb 5
import inspect
import logging
import random
import re
import reprlib
import warnings
from itertools import product
from os import cpu_count
from typing import Any, List, Optional, Union, Dict
import pkg_resources

from fugue.execution.factory import make_execution_engine
import matplotlib as mpl
import matplotlib.pyplot as plt
import matplotlib.colors as cm
import numpy as np
import pandas as pd
import polars as pl
from tqdm.autonotebook import tqdm
from triad import conditional_dispatcher
from fugue.execution.factory import try_get_context_execution_engine

from .utils import ConformalIntervals

# %% ../nbs/src/core/core.ipynb 6
if __name__ == "__main__":
    logging.basicConfig(
        format="%(asctime)s %(name)s %(levelname)s: %(message)s",
        datefmt="%Y-%m-%d %H:%M:%S",
    )
logger = logging.getLogger(__name__)

# %% ../nbs/src/core/core.ipynb 9
class GroupedArray:
    def __init__(self, data, indptr):
        self.data = data
        self.indptr = indptr
        self.n_groups = self.indptr.size - 1

    def __getitem__(self, idx):
        if isinstance(idx, int):
            return self.data[self.indptr[idx] : self.indptr[idx + 1]]
        elif isinstance(idx, slice):
            idx = slice(idx.start, idx.stop + 1, idx.step)
            new_indptr = self.indptr[idx].copy()
            new_data = self.data[new_indptr[0] : new_indptr[-1]].copy()
            new_indptr -= new_indptr[0]
            return GroupedArray(new_data, new_indptr)
        raise ValueError(f"idx must be either int or slice, got {type(idx)}")

    def __len__(self):
        return self.n_groups

    def __repr__(self):
        return f"GroupedArray(n_data={self.data.size:,}, n_groups={self.n_groups:,})"

    def __eq__(self, other):
        if not hasattr(other, "data") or not hasattr(other, "indptr"):
            return False
        return np.allclose(self.data, other.data) and np.array_equal(
            self.indptr, other.indptr
        )

    def fit(self, models):
        fm = np.full((self.n_groups, len(models)), np.nan, dtype=object)
        for i, grp in enumerate(self):
            y = grp[:, 0] if grp.ndim == 2 else grp
            X = grp[:, 1:] if (grp.ndim == 2 and grp.shape[1] > 1) else None
            for i_model, model in enumerate(models):
                new_model = model.new()
                fm[i, i_model] = new_model.fit(y=y, X=X)
        return fm

    def _get_cols(self, models, attr, h, X, level=tuple()):
        n_models = len(models)
        cuts = np.full(n_models + 1, fill_value=0, dtype=np.int32)
        has_level_models = np.full(n_models, fill_value=False, dtype=bool)
        cuts[0] = 0
        for i_model, model in enumerate(models):
            len_cols = 1  # mean
            has_level = (
                "level" in inspect.signature(getattr(model, attr)).parameters
                and len(level) > 0
            )
            has_level_models[i_model] = has_level
            if has_level:
                len_cols += 2 * len(level)  # levels
            cuts[i_model + 1] = len_cols + cuts[i_model]
        return cuts, has_level_models

    def _output_fcst(self, models, attr, h, X, level=tuple()):
        # returns empty output according to method
        cuts, has_level_models = self._get_cols(
            models=models, attr=attr, h=h, X=X, level=level
        )
        out = np.full(
            (self.n_groups * h, cuts[-1]), fill_value=np.nan, dtype=np.float32
        )
        return out, cuts, has_level_models

    def predict(self, fm, h, X=None, level=tuple()):
        # fm stands for fitted_models
        # and fm should have fitted_model
        fcsts, cuts, has_level_models = self._output_fcst(
            models=fm[0], attr="predict", h=h, X=X, level=level
        )
        matches = ["mean", "lo", "hi"]
        cols = []
        for i_model in range(fm.shape[1]):
            has_level = has_level_models[i_model]
            kwargs = {}
            if has_level:
                kwargs["level"] = level
            for i, _ in enumerate(self):
                if X is not None:
                    X_ = X[i]
                else:
                    X_ = None
                res_i = fm[i, i_model].predict(h=h, X=X_, **kwargs)
                cols_m = [
                    key
                    for key in res_i.keys()
                    if any(key.startswith(m) for m in matches)
                ]
                fcsts_i = np.vstack([res_i[key] for key in cols_m]).T
                model_name = repr(fm[i, i_model])
                cols_m = [
                    f"{model_name}" if col == "mean" else f"{model_name}-{col}"
                    for col in cols_m
                ]
                if fcsts_i.ndim == 1:
                    fcsts_i = fcsts_i[:, None]
                fcsts[i * h : (i + 1) * h, cuts[i_model] : cuts[i_model + 1]] = fcsts_i
            cols += cols_m
        return fcsts, cols

    def fit_predict(self, models, h, X=None, level=tuple()):
        # fitted models
        fm = self.fit(models=models)
        # forecasts
        fcsts, cols = self.predict(fm=fm, h=h, X=X, level=level)
        return fm, fcsts, cols

    def forecast(
        self,
        models,
        h,
        fallback_model=None,
        fitted=False,
        X=None,
        level=tuple(),
        verbose=False,
    ):
        fcsts, cuts, has_level_models = self._output_fcst(
            models=models, attr="forecast", h=h, X=X, level=level
        )
        matches = ["mean", "lo", "hi"]
        matches_fitted = ["fitted", "fitted-lo", "fitted-hi"]
        if fitted:
            # for the moment we dont return levels for fitted values in
            # forecast mode
            fitted_vals = np.full(
                (self.data.shape[0], 1 + cuts[-1]), np.nan, dtype=np.float32
            )
            if self.data.ndim == 1:
                fitted_vals[:, 0] = self.data
            else:
                fitted_vals[:, 0] = self.data[:, 0]
        iterable = tqdm(
            enumerate(self), disable=(not verbose), total=len(self), desc="Forecast"
        )
        for i, grp in iterable:
            y_train = grp[:, 0] if grp.ndim == 2 else grp
            X_train = grp[:, 1:] if (grp.ndim == 2 and grp.shape[1] > 1) else None
            if X is not None:
                X_f = X[i]
            else:
                X_f = None
            cols = []
            cols_fitted = []
            for i_model, model in enumerate(models):
                has_level = has_level_models[i_model]
                kwargs = {}
                if has_level:
                    kwargs["level"] = level
                try:
                    res_i = model.forecast(
                        h=h, y=y_train, X=X_train, X_future=X_f, fitted=fitted, **kwargs
                    )
                except Exception as error:
                    if fallback_model is not None:
                        res_i = fallback_model.forecast(
                            h=h,
                            y=y_train,
                            X=X_train,
                            X_future=X_f,
                            fitted=fitted,
                            **kwargs,
                        )
                    else:
                        raise error
                cols_m = [
                    key
                    for key in res_i.keys()
                    if any(key.startswith(m) for m in matches)
                ]
                fcsts_i = np.vstack([res_i[key] for key in cols_m]).T
                cols_m = [
                    f"{repr(model)}" if col == "mean" else f"{repr(model)}-{col}"
                    for col in cols_m
                ]
                if fcsts_i.ndim == 1:
                    fcsts_i = fcsts_i[:, None]
                fcsts[i * h : (i + 1) * h, cuts[i_model] : cuts[i_model + 1]] = fcsts_i
                cols += cols_m
                if fitted:
                    cols_m_fitted = [
                        key
                        for key in res_i.keys()
                        if any(key.startswith(m) for m in matches_fitted)
                    ]
                    fitted_i = np.vstack([res_i[key] for key in cols_m_fitted]).T
                    cols_m_fitted = [
                        f"{repr(model)}"
                        if col == "fitted"
                        else f"{repr(model)}-{col.replace('fitted-', '')}"
                        for col in cols_m_fitted
                    ]
                    fitted_vals[
                        self.indptr[i] : self.indptr[i + 1],
                        (cuts[i_model] + 1) : (cuts[i_model + 1] + 1),
                    ] = fitted_i
                    cols_fitted += cols_m_fitted
        result = {"forecasts": fcsts, "cols": cols}
        if fitted:
            result["fitted"] = {"values": fitted_vals}
            result["fitted"]["cols"] = ["y"] + cols_fitted
        return result

    def cross_validation(
        self,
        models,
        h,
        test_size,
        fallback_model=None,
        step_size=1,
        input_size=None,
        fitted=False,
        level=tuple(),
        refit=True,
        verbose=False,
    ):
        # output of size: (ts, window, h)
        if (test_size - h) % step_size:
            raise Exception("`test_size - h` should be module `step_size`")
        n_windows = int((test_size - h) / step_size) + 1
        n_models = len(models)
        cuts, has_level_models = self._get_cols(
            models=models, attr="forecast", h=h, X=None, level=level
        )
        # first column of out is the actual y
        out = np.full(
            (self.n_groups, n_windows, h, 1 + cuts[-1]), np.nan, dtype=np.float32
        )
        if fitted:
            fitted_vals = np.full(
                (self.data.shape[0], n_windows, n_models + 1), np.nan, dtype=np.float32
            )
            fitted_idxs = np.full((self.data.shape[0], n_windows), False, dtype=bool)
            last_fitted_idxs = np.full_like(fitted_idxs, False, dtype=bool)
        matches = ["mean", "lo", "hi"]
        steps = list(range(-test_size, -h + 1, step_size))
        for i_ts, grp in enumerate(self):
            iterable = tqdm(
                enumerate(steps, start=0),
                desc=f"Cross Validation Time Series {i_ts + 1}",
                disable=(not verbose),
                total=len(steps),
            )
            for i_window, cutoff in iterable:
                end_cutoff = cutoff + h
                in_size_disp = cutoff if input_size is None else input_size
                y = grp[(cutoff - in_size_disp) : cutoff]
                y_train = y[:, 0] if y.ndim == 2 else y
                X_train = y[:, 1:] if (y.ndim == 2 and y.shape[1] > 1) else None
                y_test = grp[cutoff:] if end_cutoff == 0 else grp[cutoff:end_cutoff]
                X_future = (
                    y_test[:, 1:]
                    if (y_test.ndim == 2 and y_test.shape[1] > 1)
                    else None
                )
                out[i_ts, i_window, :, 0] = y_test[:, 0] if y.ndim == 2 else y_test
                if fitted:
                    fitted_vals[self.indptr[i_ts] : self.indptr[i_ts + 1], i_window, 0][
                        (cutoff - in_size_disp) : cutoff
                    ] = y_train
                    fitted_idxs[self.indptr[i_ts] : self.indptr[i_ts + 1], i_window][
                        (cutoff - in_size_disp) : cutoff
                    ] = True
                    last_fitted_idxs[
                        self.indptr[i_ts] : self.indptr[i_ts + 1], i_window
                    ][cutoff - 1] = True
                cols = ["y"]
                for i_model, model in enumerate(models):
                    has_level = has_level_models[i_model]
                    kwargs = {}
                    if has_level:
                        kwargs["level"] = level
                    if refit:
                        try:
                            res_i = model.forecast(
                                h=h,
                                y=y_train,
                                X=X_train,
                                X_future=X_future,
                                fitted=fitted,
                                **kwargs,
                            )
                        except Exception as error:
                            if fallback_model is not None:
                                res_i = fallback_model.forecast(
                                    h=h,
                                    y=y_train,
                                    X=X_train,
                                    X_future=X_future,
                                    fitted=fitted,
                                    **kwargs,
                                )
                            else:
                                raise error
                    else:
                        if i_window == 0:
                            # for the first window we have to fit each model
                            model = model.fit(y=y_train, X=X_train)
                            if fallback_model is not None:
                                fallback_model = fallback_model.fit(
                                    y=y_train, X=X_train
                                )
                        try:
                            res_i = model.forward(
                                h=h,
                                y=y_train,
                                X=X_train,
                                X_future=X_future,
                                fitted=fitted,
                                **kwargs,
                            )
                        except Exception as error:
                            if fallback_model is not None:
                                res_i = fallback_model.forward(
                                    h=h,
                                    y=y_train,
                                    X=X_train,
                                    X_future=X_future,
                                    fitted=fitted,
                                    **kwargs,
                                )
                            else:
                                raise error
                    cols_m = [
                        key
                        for key in res_i.keys()
                        if any(key.startswith(m) for m in matches)
                    ]
                    fcsts_i = np.vstack([res_i[key] for key in cols_m]).T
                    cols_m = [
                        f"{repr(model)}" if col == "mean" else f"{repr(model)}-{col}"
                        for col in cols_m
                    ]
                    out[
                        i_ts, i_window, :, (1 + cuts[i_model]) : (1 + cuts[i_model + 1])
                    ] = fcsts_i
                    if fitted:
                        fitted_vals[
                            self.indptr[i_ts] : self.indptr[i_ts + 1],
                            i_window,
                            i_model + 1,
                        ][(cutoff - in_size_disp) : cutoff] = res_i["fitted"]
                    cols += cols_m
        result = {"forecasts": out.reshape(-1, 1 + cuts[-1]), "cols": cols}
        if fitted:
            result["fitted"] = {
                "values": fitted_vals,
                "idxs": fitted_idxs,
                "last_idxs": last_fitted_idxs,
                "cols": ["y"] + [repr(model) for model in models],
            }
        return result

    def split(self, n_chunks):
        return [
            self[x[0] : x[-1] + 1]
            for x in np.array_split(range(self.n_groups), n_chunks)
            if x.size
        ]

    def split_fm(self, fm, n_chunks):
        return [
            fm[x[0] : x[-1] + 1]
            for x in np.array_split(range(self.n_groups), n_chunks)
            if x.size
        ]

# %% ../nbs/src/core/core.ipynb 22
class DataFrameProcessing:
    """
    A utility to process Pandas or Polars dataframes for time series forecasting.

    This class ensures the dataframe is properly structured, with required columns
    ('unique_id', 'ds', 'y'), and the 'ds' column is of datetime type. It also
    provides options for sorting the dataframe based on a unique identifier and a
    timestamp, and separates the data into different arrays for easy access during
    forecasting operations.

    Attributes:
    ----------
    dataframe : pd.DataFrame or pl.DataFrame
        A pandas or polars dataframe to be processed.
    sort_dataframe : bool
        A boolean indicating whether the dataframe should be sorted.

    Methods:
    -------
    __call__():
        Processes the dataframe by ensuring the columns are in the correct format,
        sorts the dataframe if required, and separates the data into different
        arrays for future operations.
    _to_np_and_engine():
        Converts the dataframe to a numpy structured array and identifies the
        dataframe engine (pandas or polars).
    _validate_dataframe(dataframe: Union[pd.DataFrame, pl.DataFrame]):
        Checks if the required columns ('unique_id', 'ds', 'y') are present in the
        dataframe.
    _check_datetime(arr: np.array) -> np.array:
        Validates that the 'ds' column is of datetime type, and if not, attempts to
        convert it to datetime.
    """

    def __init__(
        self,
        dataframe: Union[pd.DataFrame, pl.DataFrame],
        sort_dataframe: bool,
        validate: Optional[bool] = True,
    ):
        self.dataframe = dataframe
        self.sort_dataframe = sort_dataframe
        self.validate = validate

        # Columns declaration
        self.non_value_columns: Union[tuple, list] = ["unique_id", "ds"]
        self.datetime_column_name: str = "ds"
        self.dt_dtype = np.dtype("datetime64")
        self.__call__()

    def __call__(self):
        """Sequential execution of the code"""
        # Declaring values that will be utilized
        self.np_df = self._to_np_and_engine()
        self.dataframe_columns = self.np_df.dtype.names

        # Processing value columns
        value_columns = [
            column
            for column in self.dataframe_columns
            if column not in self.non_value_columns
        ]
        self.value_array = self.np_df[value_columns]
        if self.value_array.ndim == 1 and len(value_columns) > 1:
            self.value_array = np.stack(
                [
                    self.value_array[name].astype(float)
                    for name in self.value_array.dtype.names
                ],
                axis=1,
            )
        if self.value_array.ndim == 1 and len(value_columns) == 1:
            self.value_array = (
                self.value_array[value_columns].astype(float).reshape(-1, 1)
            )

        # Processing unique_id
        self.unique_id = self.np_df["unique_id"]
        if self.unique_id.dtype.kind == "O":
            self.unique_id.astype(str)

        # If values are already int or float then they won't be converted
        if self.unique_id.dtype.kind not in ["i", "f"]:
            # If all values in the numpy array are numerical then proceed with conversion
            if np.char.isnumeric(self.unique_id.astype(str)).all():
                # If number are whole then they will be converted to `int`, else `float`
                # This is pure aesthetics addition.
                self.unique_id = self.unique_id.astype(float)
                if np.isclose(self.unique_id, np.round(self.unique_id)).all():
                    self.unique_id = self.unique_id.astype(int)
        # NOTE: When sorting with Numpy, character values may be prioritized over numerical values if the data
        # type is set to 'object'. For instance, the value '10' would come before '3' because it contains '1' and '0'
        # at the beginning. One solution to this problem is to convert the data to 'float' if it is numerical.
        unique_id_count = pd.Series(self.unique_id).value_counts(sort=False)
        self.indices, sizes = unique_id_count.index, unique_id_count.values
        cum_sizes = np.cumsum(sizes)

        # Processing datestamp
        self.dates = self.np_df[self.datetime_column_name]
        if self.engine_dataframe == pd.DataFrame:
            self.dates = self.dataframe.index.get_level_values(
                self.datetime_column_name
            )
        self.dates = self.dates[cum_sizes - 1]
        self.indptr = np.append(0, cum_sizes).astype(np.int32)

        # Index that will be used by pandas, not polars
        self.index = pd.MultiIndex.from_arrays(
            [
                self.np_df["unique_id"],
                self.np_df["ds"],
            ],
            names=["unique_id", "ds"],
        )

    def grouped_array(self):
        return GroupedArray(self.value_array, self.indptr)

    def _to_np_and_engine(self):
        """
        This function will be utilised to convert DataFrame to dictionary.

        Returns:
            tuple[pd.DataFrame or pl.DataFrame, dict]: the engine that will be used to construct
                the output DataFrame and dictionary of DataFrame values

        Raises:
            ValueError: If DataFrame engine is not supported and/or accounted for.
        """

        ####################
        # Polars DataFrame #
        ####################
        if isinstance(self.dataframe, pl.DataFrame):
            # Ensure that all required columns are present in the DataFrame:
            self.engine_dataframe = pl.DataFrame
            if self.validate:
                self._validate_dataframe(self.dataframe)
            elif self.validate == False:
                self._partial_val_df(self.dataframe)

            # datetime check
            dt_arr = self.dataframe["ds"].to_numpy()
            processed_dt_arr = self._check_datetime(dt_arr)
            if type(dt_arr) != type(processed_dt_arr):
                self.dataframe = self.dataframe.with_columns(
                    pl.from_numpy(processed_dt_arr.to_numpy(), schema=["ds"])
                )

            sample_index_df = self.dataframe[self.non_value_columns]
            sorted_index_df = sample_index_df.sort(self.non_value_columns)
            is_monotonic_increasing = sample_index_df.frame_equal(sorted_index_df)

            # Sorting will be performed if sort is set to true and values are unsorted
            if not is_monotonic_increasing and self.sort_dataframe:
                self.dataframe = self.dataframe.sort(self.non_value_columns)

            # resources: https://github.com/pola-rs/polars/blob/4fca1ae51864f74e0367d8bc91b4a2db00e54174/py-polars/polars/dataframe/frame.py#L1975
            # resources: https://numpy.org/doc/stable/user/basics.rec.html
            # resources: https://numpy.org/doc/stable/reference/generated/numpy.core.records.fromarrays.html
            # NOTE: Structured array is not available in polars under the version 0.17.12
            pl_version = pkg_resources.get_distribution("polars").version
            min_pl_v = pkg_resources.parse_version("0.17.12")
            if pkg_resources.parse_version(pl_version) >= min_pl_v:
                return self.dataframe.to_numpy(structured=True)
            else:
                arrays = []
                for column, column_dtype in self.dataframe.schema.items():
                    ser = self.dataframe[column]
                    arr = ser.to_numpy()
                    arrays.append(
                        arr.astype(str, copy=False)
                        if str(column_dtype) == "Utf8" and not ser.has_validity()
                        else arr
                    )
                arr_dtypes = list(
                    zip(self.dataframe.columns, (a.dtype for a in arrays))
                )
                return np.rec.fromarrays(arrays, dtype=np.dtype(arr_dtypes))

        ####################
        # Pandas DataFrame #
        ####################
        elif isinstance(self.dataframe, pd.DataFrame):
            self.engine_dataframe = pd.DataFrame
            # Ensure that all required columns are present in the DataFrame:
            # Full validation
            if self.validate and self.dataframe.index.name == "unique_id":
                reset_df = self.dataframe.reset_index()
                self._validate_dataframe(reset_df)
                del reset_df

            elif self.validate and self.dataframe.index.name != "unique_id":
                self._validate_dataframe(self.dataframe)
                self.dataframe = self.dataframe.set_index("unique_id")

            # Partial validation
            elif self.validate == False and self.dataframe.index.name == "unique_id":
                reset_df = self.dataframe.reset_index()
                self._partial_val_df(reset_df)
                del reset_df

            elif self.validate == False and self.dataframe.index.name != "unique_id":
                self._partial_val_df(self.dataframe)
                self.dataframe = self.dataframe.set_index("unique_id")

            # Datetime check
            dt_arr = self.dataframe["ds"].values
            self.dataframe = self.dataframe.copy(deep=False)
            self.dataframe["ds"] = self._check_datetime(dt_arr)

            self.dataframe = self.dataframe.set_index("ds", append=True)

            # Sorting will be performed if sort is set to true and values are unsorted
            if not self.dataframe.index.is_monotonic_increasing and self.sort_dataframe:
                self.dataframe = self.dataframe.sort_values(self.non_value_columns)

            np_df = self.dataframe.to_records(index=True)

            return np_df

        ####################
        # Not Supported DF #
        ####################
        else:
            raise ValueError(f"{type(self.dataframe)} is not supported")

    def _validate_dataframe(self, dataframe: Union[pd.DataFrame, pl.DataFrame]):
        """
        Will ensure that all DataFrame columns match the required columns.

        This code requires a pandas DataFrame with the following structure:

        Columns:
        - `unique_id` Union[str, int, categorical]: an identifier for the series
        - `ds` Union[datestamp, int]: column should be either an integer indexing time or a
            datestamp ideally like YYYY-MM-DD for a date or YYYY-MM-DD HH:MM:SS for a timestamp.
        - `y` Union[float, int]: represents the measurement we wish to forecast.

        Raise:
            KeyError: DataFrame is missing `unique_id`, `ds`, `y` columns.
        """
        required_columns = ["unique_id", "ds", "y"]
        matches = all(rc in dataframe.columns for rc in required_columns)
        if not matches:
            raise KeyError(
                "The DataFrame doesn't contain {} columns".format(
                    ", ".join(required_columns)
                )
            )

    def _partial_val_df(self, dataframe: Union[pd.DataFrame, pl.DataFrame]):
        """
        Will ensure that all DataFrame columns match the required columns.

        This code requires a pandas DataFrame with the following structure:

        Columns:
        - `unique_id` Union[str, int, categorical]: an identifier for the series
        - `ds` Union[datestamp, int]: column should be either an integer indexing time or a
            datestamp ideally like YYYY-MM-DD for a date or YYYY-MM-DD HH:MM:SS for a timestamp.

        Raise:
            KeyError: DataFrame is missing `unique_id` and/or `ds` columns.
        """
        required_columns = ["unique_id", "ds"]
        matches = all(rc in dataframe.columns for rc in required_columns)
        if not matches:
            raise KeyError(
                "The DataFrame doesn't contain {} columns".format(
                    ", ".join(required_columns)
                )
            )

    def _check_datetime(self, arr: np.ndarray) -> Union[pd.DatetimeIndex, np.ndarray]:
        dt_check = pd.api.types.is_datetime64_any_dtype(arr)
        int_float_check = arr.dtype.kind in ["i", "f"]
        if not dt_check and not int_float_check:
            self._ds_is_dt = True
            try:
                return pd.to_datetime(arr)
            except Exception as e:
                msg = (
                    "Failed to parse `ds` column as datetime. "
                    "Please use `pd.to_datetime` outside to fix the error. "
                    f"{e}"
                )
                raise Exception(msg) from e
        return arr

# %% ../nbs/src/core/core.ipynb 25
def _cv_dates(last_dates, freq, h, test_size, step_size=1):
    # assuming step_size = 1
    if (test_size - h) % step_size:
        raise Exception("`test_size - h` should be module `step_size`")
    n_windows = int((test_size - h) / step_size) + 1
    if len(np.unique(last_dates)) == 1:
        if issubclass(last_dates.dtype.type, np.integer):
            total_dates = np.arange(last_dates[0] - test_size + 1, last_dates[0] + 1)
            out = np.empty((h * n_windows, 2), dtype=last_dates.dtype)
            freq = 1
        else:
            total_dates = pd.date_range(end=last_dates[0], periods=test_size, freq=freq)
            out = np.empty((h * n_windows, 2), dtype="datetime64[s]")
        for i_window, cutoff in enumerate(
            range(-test_size, -h + 1, step_size), start=0
        ):
            end_cutoff = cutoff + h
            out[h * i_window : h * (i_window + 1), 0] = (
                total_dates[cutoff:]
                if end_cutoff == 0
                else total_dates[cutoff:end_cutoff]
            )
            out[h * i_window : h * (i_window + 1), 1] = np.tile(
                total_dates[cutoff] - freq, h
            )
        dates = pd.DataFrame(
            np.tile(out, (len(last_dates), 1)), columns=["ds", "cutoff"]
        )
    else:
        dates = pd.concat(
            [
                _cv_dates(np.array([ld]), freq, h, test_size, step_size)
                for ld in last_dates
            ]
        )
        dates = dates.reset_index(drop=True)
    return dates

# %% ../nbs/src/core/core.ipynb 29
def _get_n_jobs(n_groups, n_jobs):
    if n_jobs == -1 or (n_jobs is None):
        actual_n_jobs = cpu_count()
    else:
        actual_n_jobs = n_jobs
    return min(n_groups, actual_n_jobs)

# %% ../nbs/src/core/core.ipynb 32
def _parse_ds_type(df):
    dt_col = df["ds"]
    dt_check = pd.api.types.is_datetime64_any_dtype(dt_col)
    int_float_check = dt_col.dtype.kind in ["i", "f"]
    if not dt_check and not int_float_check:
        df = df.copy()
        try:
            df["ds"] = pd.to_datetime(df["ds"])
        except Exception as e:
            msg = (
                "Failed to parse `ds` column as datetime. "
                "Please use `pd.to_datetime` outside to fix the error. "
                f"{e}"
            )
            raise Exception(msg) from e
    return df

# %% ../nbs/src/core/core.ipynb 33
class _StatsForecast:
    def __init__(
        self,
        models: List[Any],
        freq: str,
        n_jobs: int = 1,
        df: Optional[Union[pd.DataFrame, pl.DataFrame]] = None,
        sort_df: bool = True,
        fallback_model: Optional[Any] = None,
        verbose: bool = False,
    ):
        """Train statistical models.

        The `StatsForecast` class allows you to efficiently fit multiple `StatsForecast` models
        for large sets of time series. It operates with pandas DataFrame `df` that identifies series
        and datestamps with the `unique_id` and `ds` columns. The `y` column denotes the target
        time series variable.

        The class has memory-efficient `StatsForecast.forecast` method that avoids storing partial
        model outputs. While the `StatsForecast.fit` and `StatsForecast.predict` methods with
        Scikit-learn interface store the fitted models.

        The `StatsForecast` class offers parallelization utilities with Dask, Spark and Ray back-ends.
        See distributed computing example [here](https://github.com/Nixtla/statsforecast/tree/main/experiments/ray).

        Parameters
        ----------
        models : List[Any]
            List of instantiated objects models.StatsForecast.
        freq : str
            Frequency of the data.
            See [pandas' available frequencies](https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases).
        n_jobs : int (default=1)
            Number of jobs used in the parallel processing, use -1 for all cores.
        df : pandas.DataFrame or pl.DataFrame, optional (default=None)
            DataFrame with columns [`unique_id`, `ds`, `y`] and exogenous.
        sort_df : bool (default=True)
            If True, sort `df` by [`unique_id`,`ds`].
        fallback_model : Any, optional (default=None)
            Model to be used if a model fails.
            Only works with the `forecast` and `cross_validation` methods.
        verbose : bool (default=True)
            Prints TQDM progress bar when `n_jobs=1`.
        """

        # TODO @fede: needed for residuals, think about it later
        self.models = models
        self._validate_model_names()
        self.freq = pd.tseries.frequencies.to_offset(freq)
        self.n_jobs = n_jobs
        self.fallback_model = fallback_model
        self.verbose = verbose
        self.n_jobs == 1
        self._prepare_fit(df=df, sort_df=sort_df)

    def _validate_model_names(self):
        # Some test models don't have alias
        names = [getattr(model, "alias", lambda: None) for model in self.models]
        names = [x for x in names if x is not None]
        if len(names) != len(set(names)):
            raise ValueError(
                "Model names must be unique. You can use `alias` to set a unique name for each model."
            )

    def _prepare_fit(self, df, sort_df):
        if df is not None:
            df_process = DataFrameProcessing(df, sort_df)
            self.ga = df_process.grouped_array()
            self.uids = df_process.indices
            self.last_dates = df_process.dates
            self.ds = df_process.index
            self.og_dates = df_process.np_df["ds"]
            self.og_unique_id = df_process.np_df["unique_id"]
            self.engine = df_process.engine_dataframe
            self.n_jobs = _get_n_jobs(len(self.ga), self.n_jobs)
            self.sort_df = sort_df

    def _set_prediction_intervals(self, prediction_intervals):
        for model in self.models:
            interval = getattr(model, "prediction_intervals", None)
            if interval is None:
                setattr(model, "prediction_intervals", prediction_intervals)

    def fit(
        self,
        df: Optional[Union[pd.DataFrame, pl.DataFrame]] = None,
        sort_df: bool = True,
        prediction_intervals: Optional[ConformalIntervals] = None,
    ):
        """Fit statistical models.

        Fit `models` to a large set of time series from DataFrame `df`
        and store fitted models for later inspection.

        Parameters
        ----------
        df : pandas.DataFrame or polars.DataFrame, optional (default=None)
            DataFrame with columns [`unique_id`, `ds`, `y`] and exogenous.
            If None, the `StatsForecast` class should have been instantiated
            using `df`.
        sort_df : bool (default=True)
            If True, sort `df` by [`unique_id`,`ds`].
        prediction_intervals : ConformalIntervals, optional (default=None)
            Configuration to calibrate prediction intervals (Conformal Prediction).

        Returns
        -------
        self : StatsForecast
            Returns with stored `StatsForecast` fitted `models`.
        """
        self._set_prediction_intervals(prediction_intervals=prediction_intervals)
        self._prepare_fit(df, sort_df)
        if self.n_jobs == 1:
            self.fitted_ = self.ga.fit(models=self.models)
        else:
            self.fitted_ = self._fit_parallel()
        return self

    def _make_future_df(self, h: int):
        if issubclass(self.last_dates.dtype.type, np.integer):
            last_date_f = lambda x: np.arange(
                x + 1, x + 1 + h, dtype=self.last_dates.dtype
            )
        else:
            last_date_f = lambda x: pd.date_range(
                x + self.freq, periods=h, freq=self.freq
            )
        if len(np.unique(self.last_dates)) == 1:
            dates = np.tile(last_date_f(self.last_dates[0]), len(self.ga))
        else:
            dates = np.hstack([last_date_f(last_date) for last_date in self.last_dates])
        u_id_ser: Union[pd.Series, pl.Series] = np.repeat(self.uids, h)
        unique_id: np.ndarray = u_id_ser.to_numpy()

        # In older versions to_numpy converts string values into object,
        # creating bytes error, this fixes it
        if unique_id.dtype.kind == "O":
            unique_id = unique_id.astype(str)

        if self.engine == pd.DataFrame:
            idx = pd.Index(unique_id, name="unique_id")
            df = self.engine({"ds": dates}, index=idx)
        elif self.engine == pl.DataFrame:
            df = self.engine({"unique_id": unique_id, "ds": dates})
        return df

    def _parse_X_level(self, h, X, level):
        if X is not None:
            if isinstance(X, pd.DataFrame):
                if X.index.name != "unique_id":
                    X = X.set_index("unique_id")
            expected_shape_rows = h * len(self.ga)
            ga_shape = self.ga.data.shape[1]
            # Polars doesn't have index, hence, extra "column"
            expected_shape_cols = (
                ga_shape if not isinstance(X, pl.DataFrame) else ga_shape + 1
            )
            expected_shape = (expected_shape_rows, expected_shape_cols)

            if X.shape != expected_shape:
                raise ValueError(
                    f"Expected X to have shape {expected_shape}, but got {X.shape}"
                )
            X = DataFrameProcessing(
                X, sort_dataframe=self.sort_df, validate=False
            ).grouped_array()
        if level is None:
            level = tuple()
        return X, level

    def predict(
        self,
        h: int,
        X_df: Optional[Union[pd.DataFrame, pl.DataFrame]] = None,
        level: Optional[List[int]] = None,
    ):
        """Predict statistical models.

        Use stored fitted `models` to predict large set of time series from DataFrame `df`.

        Parameters
        ----------
        h : int
            Forecast horizon.
        X_df : pandas.DataFrame | polars.DataFrame, optional (default=None)
            DataFrame with [`unique_id`, `ds`] columns and `df`'s future exogenous.
        level : List[float], optional (default=None)
            Confidence levels between 0 and 100 for prediction intervals.

        Returns
        -------
        fcsts_df : pandas.DataFrame | polars.DataFrame
            DataFrame with `models` columns for point predictions and probabilistic
            predictions for all fitted `models`.
        """

        if (
            any(
                getattr(m, "prediction_intervals", None) is not None
                for m in self.models
            )
            and level is None
        ):
            warnings.warn(
                "Prediction intervals are set but `level` was not provided. "
                "Predictions won't have intervals."
            )
        X, level = self._parse_X_level(h=h, X=X_df, level=level)
        if self.n_jobs == 1:
            fcsts, cols = self.ga.predict(fm=self.fitted_, h=h, X=X, level=level)
        else:
            fcsts, cols = self._predict_parallel(h=h, X=X, level=level)
        fcsts_df = self._make_future_df(h=h)
        fcsts_df[cols] = fcsts
        return fcsts_df

    def fit_predict(
        self,
        h: int,
        df: Optional[Union[pd.DataFrame, pl.DataFrame]] = None,
        X_df: Optional[Union[pd.DataFrame, pl.DataFrame]] = None,
        level: Optional[List[int]] = None,
        sort_df: bool = True,
        prediction_intervals: Optional[ConformalIntervals] = None,
    ):
        """Fit and Predict with statistical models.

        This method avoids memory burden due from object storage.
        It is analogous to Scikit-Learn `fit_predict` without storing information.
        It requires the forecast horizon `h` in advance.

        In contrast to `StatsForecast.forecast` this method stores partial models outputs.

        Parameters
        ----------
        h : int
            Forecast horizon.
        df : pandas.DataFrame | polars.DataFrame, optional (default=None)
            DataFrame with columns [`unique_id`, `ds`, `y`] and exogenous variables.
            If None, the `StatsForecast` class should have been instantiated
            using `df`.
        X_df : pandas.DataFrame | polars.DataFrame, optional (default=None)
            DataFrame with [`unique_id`, `ds`] columns and `df`'s future exogenous.
        level : List[float], optional (default=None)
            Confidence levels between 0 and 100 for prediction intervals.
        sort_df : bool (default=True)
            If True, sort `df` by [`unique_id`,`ds`].
        prediction_intervals : ConformalIntervals, optional (default=None)
            Configuration to calibrate prediction intervals (Conformal Prediction).

        Returns
        -------
        fcsts_df : pandas.DataFrame | polars.DataFrame
            DataFrame with `models` columns for point predictions and probabilistic
            predictions for all fitted `models`.
        """
        if prediction_intervals is not None and level is None:
            raise ValueError(
                "You must specify `level` when using `prediction_intervals`"
            )
        self._set_prediction_intervals(prediction_intervals=prediction_intervals)
        self._prepare_fit(df, sort_df)
        X, level = self._parse_X_level(h=h, X=X_df, level=level)
        if self.n_jobs == 1:
            self.fitted_, fcsts, cols = self.ga.fit_predict(
                models=self.models, h=h, X=X, level=level
            )
        else:
            self.fitted_, fcsts, cols = self._fit_predict_parallel(
                h=h, X=X, level=level
            )
        fcsts_df = self._make_future_df(h=h)
        fcsts_df[cols] = fcsts
        return fcsts_df

    def forecast(
        self,
        h: int,
        df: Optional[Union[pd.DataFrame, pl.DataFrame]] = None,
        X_df: Optional[Union[pd.DataFrame, pl.DataFrame]] = None,
        level: Optional[List[int]] = None,
        fitted: bool = False,
        sort_df: bool = True,
        prediction_intervals: Optional[ConformalIntervals] = None,
    ):
        """Memory Efficient predictions.

        This method avoids memory burden due from object storage.
        It is analogous to Scikit-Learn `fit_predict` without storing information.
        It requires the forecast horizon `h` in advance.

        Parameters
        ----------
        h : int
            Forecast horizon.
        df : pandas.DataFrame | polars.DataFrame, optional (default=None)
            DataFrame with columns [`unique_id`, `ds`, `y`] and exogenous.
            If None, the `StatsForecast` class should have been instantiated
            using `df`.
        X_df : pandas.DataFrame | polars.DataFrame, optional (default=None)
            DataFrame with [`unique_id`, `ds`] columns and `df`'s future exogenous.
        level : List[float], optional (default=None)
            Confidence levels between 0 and 100 for prediction intervals.
        fitted : bool (default=False)
            Wether or not return insample predictions.
        sort_df : bool (default=True)
            If True, sort `df` by [`unique_id`,`ds`].
        prediction_intervals : ConformalIntervals, optional (default=None)
            Configuration to calibrate prediction intervals (Conformal Prediction).

        Returns
        -------
        fcsts_df : pandas.DataFrame | polars.DataFrame
            DataFrame with `models` columns for point predictions and probabilistic
            predictions for all fitted `models`.
        """
        self._set_prediction_intervals(prediction_intervals=prediction_intervals)
        self._prepare_fit(df, sort_df)
        X, level = self._parse_X_level(h=h, X=X_df, level=level)
        if self.n_jobs == 1:
            res_fcsts = self.ga.forecast(
                models=self.models,
                h=h,
                fallback_model=self.fallback_model,
                fitted=fitted,
                X=X,
                level=level,
                verbose=self.verbose,
            )
        else:
            res_fcsts = self._forecast_parallel(h=h, fitted=fitted, X=X, level=level)
        if fitted:
            self.fcst_fitted_values_ = res_fcsts["fitted"]
        fcsts = res_fcsts["forecasts"]
        cols = res_fcsts["cols"]
        fcsts_df = self._make_future_df(h=h)
        fcsts_df[cols] = fcsts
        return fcsts_df

    def forecast_fitted_values(self):
        """Access insample predictions.

        After executing `StatsForecast.forecast`, you can access the insample
        prediction values for each model. To get them, you need to pass `fitted=True`
        to the `StatsForecast.forecast` method and then use the
        `StatsForecast.forecast_fitted_values` method.

        Parameters
        ----------
        self : StatsForecast

        Returns
        -------
        fcsts_df : pandas.DataFrame | polars.DataFrame
            DataFrame with insample `models` columns for point predictions and probabilistic
            predictions for all fitted `models`.
        """
        if not hasattr(self, "fcst_fitted_values_"):
            raise Exception("Please run `forecast` mehtod using `fitted=True`")
        cols = self.fcst_fitted_values_["cols"]
        if self.engine == pd.DataFrame:
            df = self.engine(
                self.fcst_fitted_values_["values"], columns=cols, index=self.ds
            ).reset_index(level=1)
        elif self.engine == pl.DataFrame:
            df = self.engine({"unique_id": self.og_unique_id, "ds": self.og_dates})
            df[cols] = self.fcst_fitted_values_["values"]
        return df

    def cross_validation(
        self,
        h: int,
        df: Optional[Union[pd.DataFrame, pl.DataFrame]] = None,
        n_windows: int = 1,
        step_size: int = 1,
        test_size: Optional[int] = None,
        input_size: Optional[int] = None,
        level: Optional[List[int]] = None,
        fitted: bool = False,
        refit: bool = True,
        sort_df: bool = True,
        prediction_intervals: Optional[ConformalIntervals] = None,
    ):
        """Temporal Cross-Validation.

        Efficiently fits a list of `StatsForecast`
        models through multiple training windows, in either chained or rolled manner.

        `StatsForecast.models`' speed allows to overcome this evaluation technique
        high computational costs. Temporal cross-validation provides better model's
        generalization measurements by increasing the test's length and diversity.

        Parameters
        ----------
        h : int
            Forecast horizon.
        df : pandas.DataFrame | polars.DataFrame, optional (default=None)
            DataFrame with columns [`unique_id`, `ds`, `y`] and exogenous.
            If None, the `StatsForecast` class should have been instantiated
            using `df`.
        n_windows : int (default=1)
            Number of windows used for cross validation.
        step_size : int (default=1)
            Step size between each window.
        test_size : int, optional (default=None)
            Length of test size. If passed, set `n_windows=None`.
        input_size : int, optional (default=None)
            Input size for each window, if not none rolled windows.
        level : List[float], optional (default=None)
            Confidence levels between 0 and 100 for prediction intervals.
        fitted : bool (default=False)
            Wether or not returns insample predictions.
        refit : bool (default=True)
            Wether or not refit the model for each window.
        sort_df : bool (default=True)
            If True, sort `df` by `unique_id` and `ds`.
        prediction_intervals : ConformalIntervals, optional (default=None)
            Configuration to calibrate prediction intervals (Conformal Prediction).

        Returns
        -------
        fcsts_df : pandas.DataFrame
            DataFrame with insample `models` columns for point predictions and probabilistic
            predictions for all fitted `models`.
        """
        if test_size is None:
            test_size = h + step_size * (n_windows - 1)
        elif n_windows is None:
            if (test_size - h) % step_size:
                raise Exception("`test_size - h` should be module `step_size`")
            n_windows = int((test_size - h) / step_size) + 1
        elif (n_windows is None) and (test_size is None):
            raise Exception("you must define `n_windows` or `test_size`")
        else:
            raise Exception("you must define `n_windows` or `test_size` but not both")
        if prediction_intervals is not None and level is None:
            raise ValueError(
                "You must specify `level` when using `prediction_intervals`"
            )
        self._set_prediction_intervals(prediction_intervals=prediction_intervals)
        self._prepare_fit(df, sort_df)
        series_sizes = np.diff(self.ga.indptr)
        short_series = series_sizes <= test_size
        if short_series.any():
            short_ids = self.uids[short_series].tolist()
            raise ValueError(
                f"The following series are too short for the cross validation settings: {reprlib.repr(short_ids)}\n"
                "Please remove these series or change the settings, e.g. reducing the horizon or the number of windows."
            )
        _, level = self._parse_X_level(h=h, X=None, level=level)
        if self.n_jobs == 1:
            res_fcsts = self.ga.cross_validation(
                models=self.models,
                h=h,
                test_size=test_size,
                fallback_model=self.fallback_model,
                step_size=step_size,
                input_size=input_size,
                fitted=fitted,
                level=level,
                verbose=self.verbose,
                refit=refit,
            )
        else:
            res_fcsts = self._cross_validation_parallel(
                h=h,
                test_size=test_size,
                step_size=step_size,
                input_size=input_size,
                fitted=fitted,
                level=level,
                refit=refit,
            )

        if fitted:
            self.cv_fitted_values_ = res_fcsts["fitted"]
            self.n_cv_ = n_windows

        fcsts = res_fcsts["forecasts"]
        cols = res_fcsts["cols"]
        fcsts_df = _cv_dates(
            last_dates=self.last_dates,
            freq=self.freq,
            h=h,
            test_size=test_size,
            step_size=step_size,
        )
        idx = pd.Index(np.repeat(self.uids, h * n_windows), name="unique_id")
        fcsts_df.index = idx
        fcsts_df[cols] = fcsts
        if self.engine == pl.DataFrame:
            fcsts_df = pl.from_pandas(fcsts_df, include_index=True)
        return fcsts_df

    def cross_validation_fitted_values(self):
        """Access insample cross validated predictions.

        After executing `StatsForecast.cross_validation`, you can access the insample
        prediction values for each model and window. To get them, you need to pass `fitted=True`
        to the `StatsForecast.cross_validation` method and then use the
        `StatsForecast.cross_validation_fitted_values` method.

        Parameters
        ----------
        self : StatsForecast

        Returns
        -------
        fcsts_df : pandas.DataFrame | polars.DataFrame
            DataFrame with insample `models` columns for point predictions
            and probabilistic predictions for all fitted `models`.
        """
        if not hasattr(self, "cv_fitted_values_"):
            raise Exception("Please run `cross_validation` mehtod using `fitted=True`")
        index = pd.MultiIndex.from_tuples(
            np.tile(self.ds, self.n_cv_), names=["unique_id", "ds"]
        )
        df = pd.DataFrame(index=index)
        df["cutoff"] = self.cv_fitted_values_["last_idxs"].flatten(order="F")
        df[self.cv_fitted_values_["cols"]] = np.reshape(
            self.cv_fitted_values_["values"], (-1, len(self.models) + 1), order="F"
        )
        idxs = self.cv_fitted_values_["idxs"].flatten(order="F")
        df = df.iloc[idxs].reset_index(level=1)
        df["cutoff"] = df["ds"].where(df["cutoff"]).bfill()

        if self.engine == pl.DataFrame:
            df = pl.from_pandas(df, include_index=True)
        return df

    def _get_pool(self):
        from multiprocessing import Pool

        pool_kwargs = dict()
        return Pool, pool_kwargs

    def _fit_parallel(self):
        gas = self.ga.split(self.n_jobs)
        Pool, pool_kwargs = self._get_pool()
        with Pool(self.n_jobs, **pool_kwargs) as executor:
            futures = []
            for ga in gas:
                future = executor.apply_async(ga.fit, (self.models,))
                futures.append(future)
            fm = np.vstack([f.get() for f in futures])
        return fm

    def _get_gas_Xs(self, X):
        gas = self.ga.split(self.n_jobs)
        if X is not None:
            Xs = X.split(self.n_jobs)
        else:
            from itertools import repeat

            Xs = repeat(None)
        return gas, Xs

    def _predict_parallel(self, h, X, level):
        # create elements for each core
        gas, Xs = self._get_gas_Xs(X=X)
        fms = self.ga.split_fm(self.fitted_, self.n_jobs)
        Pool, pool_kwargs = self._get_pool()
        # compute parallel forecasts
        with Pool(self.n_jobs, **pool_kwargs) as executor:
            futures = []
            for ga, fm, X_ in zip(gas, fms, Xs):
                future = executor.apply_async(
                    ga.predict,
                    (
                        fm,
                        h,
                        X_,
                        level,
                    ),
                )
                futures.append(future)
            out = [f.get() for f in futures]
            fcsts, cols = list(zip(*out))
            fcsts = np.vstack(fcsts)
            cols = cols[0]
        return fcsts, cols

    def _fit_predict_parallel(self, h, X, level):
        # create elements for each core
        gas, Xs = self._get_gas_Xs(X=X)
        Pool, pool_kwargs = self._get_pool()
        # compute parallel forecasts
        with Pool(self.n_jobs, **pool_kwargs) as executor:
            futures = []
            for ga, X_ in zip(gas, Xs):
                future = executor.apply_async(
                    ga.fit_predict,
                    (
                        self.models,
                        h,
                        X_,
                        level,
                    ),
                )
                futures.append(future)
            out = [f.get() for f in futures]
            fm, fcsts, cols = list(zip(*out))
            fm = np.vstack(fm)
            fcsts = np.vstack(fcsts)
            cols = cols[0]
        return fm, fcsts, cols

    def _forecast_parallel(self, h, fitted, X, level):
        # create elements for each core
        gas, Xs = self._get_gas_Xs(X=X)
        Pool, pool_kwargs = self._get_pool()
        # compute parallel forecasts
        result = {}
        with Pool(self.n_jobs, **pool_kwargs) as executor:
            futures = []
            for ga, X_ in zip(gas, Xs):
                future = executor.apply_async(
                    ga.forecast,
                    (
                        self.models,
                        h,
                        self.fallback_model,
                        fitted,
                        X_,
                        level,
                    ),
                )
                futures.append(future)
            out = [f.get() for f in futures]
            fcsts = [d["forecasts"] for d in out]
            fcsts = np.vstack(fcsts)
            cols = out[0]["cols"]
            result["forecasts"] = fcsts
            result["cols"] = cols
            if fitted:
                result["fitted"] = {}
                fitted_vals = [d["fitted"]["values"] for d in out]
                result["fitted"]["values"] = np.vstack(fitted_vals)
                result["fitted"]["cols"] = out[0]["fitted"]["cols"]
        return result

    def _cross_validation_parallel(
        self, h, test_size, step_size, input_size, fitted, level, refit
    ):
        # create elements for each core
        gas = self.ga.split(self.n_jobs)
        Pool, pool_kwargs = self._get_pool()
        # compute parallel forecasts
        result = {}
        with Pool(self.n_jobs, **pool_kwargs) as executor:
            futures = []
            for ga in gas:
                future = executor.apply_async(
                    ga.cross_validation,
                    (
                        self.models,
                        h,
                        test_size,
                        self.fallback_model,
                        step_size,
                        input_size,
                        fitted,
                        level,
                        refit,
                    ),
                )
                futures.append(future)
            out = [f.get() for f in futures]
            fcsts = [d["forecasts"] for d in out]
            fcsts = np.vstack(fcsts)
            cols = out[0]["cols"]
            result["forecasts"] = fcsts
            result["cols"] = cols
            if fitted:
                result["fitted"] = {}
                result["fitted"]["values"] = np.concatenate(
                    [d["fitted"]["values"] for d in out]
                )
                for key in ["last_idxs", "idxs"]:
                    result["fitted"][key] = np.concatenate(
                        [d["fitted"][key] for d in out]
                    )
                result["fitted"]["cols"] = out[0]["fitted"]["cols"]
        return result

    @staticmethod
    def plot(
        df: Union[pd.DataFrame, pl.DataFrame],
        forecasts_df: Optional[Union[pd.DataFrame, pl.DataFrame]] = None,
        unique_ids: Union[Optional[List[str]], np.ndarray] = None,
        plot_random: bool = True,
        models: Optional[List[str]] = None,
        level: Optional[List[float]] = None,
        max_insample_length: Optional[int] = None,
        plot_anomalies: bool = False,
        engine: str = "matplotlib",
        resampler_kwargs: Optional[Dict] = None,
    ):
        """Plot forecasts and insample values.

        Parameters
        ----------
        df : pandas.DataFrame
            DataFrame with columns [`unique_id`, `ds`, `y`].
        forecasts_df : pandas.DataFrame, optional (default=None)
            DataFrame with columns [`unique_id`, `ds`] and models.
        unique_ids : List[str], optional (default=None)
            Time Series to plot.
            If None, time series are selected randomly.
        plot_random : bool (default=True)
            Select time series to plot randomly.
        models : List[str], optional (default=None)
            List of models to plot.
        level : List[float], optional (default=None)
            List of prediction intervals to plot if paseed.
        max_insample_length : int, optional (default=None)
            Max number of train/insample observations to be plotted.
        plot_anomalies : bool (default=False)
            Plot anomalies for each prediction interval.
        engine : str (default='matplotlib')
            Library used to plot. 'plotly', 'plotly-resampler' or 'matplotlib'.
        resampler_kwargs : dict
            Kwargs to be passed to plotly-resampler constructor.
            For further custumization ("show_dash") call the method,
            store the plotting object and add the extra arguments to
            its `show_dash` method.
        """

        if isinstance(df, pl.DataFrame):
            df = df.to_pandas()
        if isinstance(forecasts_df, pl.DataFrame):
            forecasts_df = forecasts_df.to_pandas()

        if level is not None and not isinstance(level, list):
            raise Exception(
                "Please use a list for the `level` argument "
                "If you only have one level, use `level=[your_level]`"
            )

        if unique_ids is None:
            df_pt = DataFrameProcessing(
                dataframe=df, sort_dataframe=True, validate=False
            )
            uids_arr: pd.Index = df_pt.indices
            uid_dtype = uids_arr.dtype

            if df.index.name != "unique_id":
                df["unique_id"] = df["unique_id"].astype(uid_dtype)
                df = df.set_index("unique_id")
            else:
                df.index = df.index.astype(uid_dtype)

            if forecasts_df is not None:
                if isinstance(forecasts_df, pl.DataFrame):
                    forecasts_df = forecasts_df.to_pandas()

                if forecasts_df.index.name == "unique_id":
                    forecasts_df.index = forecasts_df.index.astype(uid_dtype)
                    unique_ids = np.intersect1d(uids_arr, forecasts_df.index.unique())
                else:
                    forecasts_df["unique_id"] = forecasts_df["unique_id"].astype(
                        uid_dtype
                    )
                    unique_ids = np.intersect1d(
                        uids_arr, forecasts_df["unique_id"].unique()
                    )
            else:
                unique_ids = uids_arr

        if plot_random:
            unique_ids = random.sample(list(unique_ids), k=min(8, len(unique_ids)))
        else:
            unique_ids = unique_ids[:8]

        if engine in ["plotly", "plotly-resampler"]:
            try:
                import plotly.graph_objects as go
                from plotly.subplots import make_subplots
            except ImportError:
                raise ImportError(
                    "plotly is not installed. "
                    "Please install it with `pip install statsforecast[plotly]`"
                )

            n_rows = min(4, len(unique_ids) // 2 + 1 if len(unique_ids) > 2 else 1)
            fig = make_subplots(
                rows=n_rows,
                cols=2 if len(unique_ids) >= 2 else 1,
                vertical_spacing=0.1,
                horizontal_spacing=0.07,
                x_title="Datestamp [ds]",
                y_title="Target [y]",
                subplot_titles=[str(uid) for uid in unique_ids],
            )
            if engine == "plotly-resampler":
                try:
                    from plotly_resampler import FigureResampler
                except ImportError:
                    raise ImportError(
                        "plotly-resampler is not installed. "
                        "Please install it with `pip install plotly-resampler`"
                    )
                resampler_kwargs = {} if resampler_kwargs is None else resampler_kwargs
                fig = FigureResampler(fig, **resampler_kwargs)
            showed_legends: set = set()

            def plotly(
                df,
                fig,
                n_rows,
                unique_ids,
                models,
                plot_anomalies,
                max_insample_length,
                showed_legends,
            ):
                if models is None:
                    exclude_str = ["lo", "hi", "unique_id", "ds"]
                    models = [
                        c
                        for c in df.columns
                        if all(item not in c for item in exclude_str)
                    ]
                if "y" not in models:
                    models = ["y"] + models
                for uid, (idx, idy) in zip(
                    unique_ids, product(range(1, n_rows + 1), range(1, 2 + 1))
                ):
                    df_uid = df.query("unique_id == @uid")
                    if max_insample_length:
                        df_uid = df_uid.iloc[-max_insample_length:]
                    plot_anomalies = "y" in df_uid and plot_anomalies
                    df_uid = _parse_ds_type(df_uid)
                    if pkg_resources.parse_version(
                        mpl.__version__
                    ) < pkg_resources.parse_version("3.6"):
                        colors = plt.cm.get_cmap("tab20b", len(models))
                    else:
                        colors = mpl.colormaps["tab20b"].resampled(len(models))
                    colors = ["#1f77b4"] + [
                        cm.to_hex(colors(i)) for i in range(len(models))
                    ]
                    for col, color in zip(models, colors):
                        if col in df_uid:
                            model = df_uid[col]
                            fig.add_trace(
                                go.Scatter(
                                    x=df_uid["ds"],
                                    y=model,
                                    mode="lines",
                                    name=col,
                                    legendgroup=col,
                                    line=dict(color=color, width=1),
                                    showlegend=(
                                        idx == 1
                                        and idy == 1
                                        and col not in showed_legends
                                    ),
                                ),
                                row=idx,
                                col=idy,
                            )
                            showed_legends.add(col)
                        model_has_level = any(f"{col}-lo" in c for c in df_uid)
                        if level is not None and model_has_level:
                            level_ = level
                        elif model_has_level:
                            level_col = df_uid.filter(like=f"{col}-lo").columns[0]
                            level_col = re.findall(
                                "[\d]+[.,\d]+|[\d]*[.][\d]+|[\d]+", level_col
                            )[0]
                            level_ = [level_col]
                        else:
                            level_ = []
                        ds = df_uid["ds"]
                        for lv in level_:
                            lo = df_uid[f"{col}-lo-{lv}"]
                            hi = df_uid[f"{col}-hi-{lv}"]
                            plot_name = f"{col}_level_{lv}"
                            fig.add_trace(
                                go.Scatter(
                                    x=np.concatenate([ds, ds[::-1]]),
                                    y=np.concatenate([hi, lo[::-1]]),
                                    fill="toself",
                                    mode="lines",
                                    fillcolor=color,
                                    opacity=-float(lv) / 100 + 1,
                                    name=plot_name,
                                    legendgroup=plot_name,
                                    line=dict(color=color, width=1),
                                    showlegend=(
                                        idx == 1
                                        and idy == 1
                                        and plot_name not in showed_legends
                                    ),
                                ),
                                row=idx,
                                col=idy,
                            )
                            showed_legends.add(plot_name)
                            if col != "y" and plot_anomalies:
                                anomalies = (df_uid["y"] < lo) | (df_uid["y"] > hi)
                                plot_name = f"{col}_anomalies_level_{lv}"
                                fig.add_trace(
                                    go.Scatter(
                                        x=ds[anomalies],
                                        y=df_uid["y"][anomalies],
                                        fillcolor=color,
                                        mode="markers",
                                        opacity=float(lv) / 100,
                                        name=plot_name,
                                        legendgroup=plot_name,
                                        line=dict(color=color, width=0.7),
                                        marker=dict(
                                            size=4, line=dict(color="red", width=0.5)
                                        ),
                                        showlegend=(
                                            idx == 1
                                            and idy == 1
                                            and plot_name not in showed_legends
                                        ),
                                    ),
                                    row=idx,
                                    col=idy,
                                )
                                showed_legends.add(plot_name)
                return fig

            fig = plotly(
                df=df,
                fig=fig,
                n_rows=n_rows,
                unique_ids=unique_ids,
                models=models,
                plot_anomalies=plot_anomalies,
                max_insample_length=max_insample_length,
                showed_legends=showed_legends,
            )
            if forecasts_df is not None:
                fig = plotly(
                    df=forecasts_df,
                    fig=fig,
                    n_rows=n_rows,
                    unique_ids=unique_ids,
                    models=models,
                    plot_anomalies=plot_anomalies,
                    max_insample_length=None,
                    showed_legends=showed_legends,
                )
            fig.update_xaxes(matches=None, showticklabels=True, visible=True)
            fig.update_layout(margin=dict(l=60, r=10, t=20, b=50))
            fig.update_layout(template="plotly_white", font=dict(size=10))
            fig.update_annotations(font_size=10)
            fig.update_layout(autosize=True, height=150 * n_rows)

        elif engine == "matplotlib":
            if len(unique_ids) == 1:
                fig, axes = plt.subplots(figsize=(24, 3.5))
                axes = np.array([[axes]])
                n_cols = 1
            else:
                n_cols = min(4, len(unique_ids) // 2 + 1 if len(unique_ids) > 2 else 1)
                fig, axes = plt.subplots(n_cols, 2, figsize=(24, 3.5 * n_cols))
                if n_cols == 1:
                    axes = np.array([axes])

            for uid, (idx, idy) in zip(unique_ids, product(range(n_cols), range(2))):
                train_uid = df.query("unique_id == @uid")
                train_uid = _parse_ds_type(train_uid)
                if max_insample_length is not None:
                    train_uid = train_uid.iloc[-max_insample_length:]
                ds = train_uid["ds"]
                y = train_uid["y"]
                axes[idx, idy].plot(ds, y, label="y")
                if forecasts_df is not None:
                    if models is None:
                        exclude_str = ["lo", "hi", "unique_id", "ds"]
                        models = [
                            c
                            for c in forecasts_df.columns
                            if all(item not in c for item in exclude_str)
                        ]
                    if "y" not in models:
                        models = ["y"] + models
                    test_uid = forecasts_df.query("unique_id == @uid")
                    plot_anomalies = "y" in test_uid and plot_anomalies
                    test_uid = _parse_ds_type(test_uid)
                    first_ds_fcst = test_uid["ds"].min()
                    axes[idx, idy].axvline(
                        x=first_ds_fcst,
                        color="black",
                        label="First ds Forecast",
                        linestyle="--",
                    )

                    if pkg_resources.parse_version(
                        mpl.__version__
                    ) < pkg_resources.parse_version("3.6"):
                        colors = plt.cm.get_cmap("tab20b", len(models))
                    else:
                        colors = mpl.colormaps["tab20b"].resampled(len(models))
                    colors = ["blue"] + [colors(i) for i in range(len(models))]
                    for col, color in zip(models, colors):
                        if col in test_uid:
                            axes[idx, idy].plot(
                                test_uid["ds"], test_uid[col], label=col, color=color
                            )
                        model_has_level = any(f"{col}-lo" in c for c in test_uid)
                        if level is not None and model_has_level:
                            level_ = level
                        elif model_has_level:
                            level_col = test_uid.filter(like=f"{col}-lo").columns[0]
                            level_col = re.findall(
                                "[\d]+[.,\d]+|[\d]*[.][\d]+|[\d]+", level_col
                            )[0]
                            level_ = [level_col]
                        else:
                            level_ = []
                        for lv in level_:
                            ds_test = test_uid["ds"]
                            lo = test_uid[f"{col}-lo-{lv}"]
                            hi = test_uid[f"{col}-hi-{lv}"]
                            axes[idx, idy].fill_between(
                                ds_test,
                                lo,
                                hi,
                                alpha=-float(lv) / 100 + 1,
                                color=color,
                                label=f"{col}_level_{lv}",
                            )
                            if col != "y" and plot_anomalies:
                                anomalies = (test_uid["y"] < lo) | (test_uid["y"] > hi)
                                axes[idx, idy].scatter(
                                    x=ds_test[anomalies],
                                    y=test_uid["y"][anomalies],
                                    color=color,
                                    s=30,
                                    alpha=float(lv) / 100,
                                    label=f"{col}_anomalies_level_{lv}",
                                    linewidths=0.5,
                                    edgecolors="red",
                                )

                axes[idx, idy].set_title(f"{uid}")
                axes[idx, idy].set_xlabel("Datestamp [ds]")
                axes[idx, idy].set_ylabel("Target [y]")
                axes[idx, idy].legend(loc="upper left")
                axes[idx, idy].xaxis.set_major_locator(
                    plt.MaxNLocator(min(len(df) // 30, 10))
                )
                axes[idx, idy].grid()
            fig.subplots_adjust(hspace=0.5)
            plt.close(fig)
        else:
            raise Exception(f"Unkwown plot engine {engine}")
        return fig

    def __repr__(self):
        return f"StatsForecast(models=[{','.join(map(repr, self.models))}])"

# %% ../nbs/src/core/core.ipynb 34
class ParallelBackend:
    def forecast(self, df, models, freq, fallback_model=None, **kwargs: Any) -> Any:
        model = _StatsForecast(
            df=df, models=models, freq=freq, fallback_model=fallback_model
        )
        return model.forecast(**kwargs)

    def cross_validation(
        self, df, models, freq, fallback_model=None, **kwargs: Any
    ) -> Any:
        model = _StatsForecast(
            df=df, models=models, freq=freq, fallback_model=fallback_model
        )
        return model.cross_validation(**kwargs)


@conditional_dispatcher
def make_backend(obj: Any, *args: Any, **kwargs: Any) -> ParallelBackend:
    return ParallelBackend()

# %% ../nbs/src/core/core.ipynb 35
class StatsForecast(_StatsForecast):
    """Train statistical models.

    The `StatsForecast` class allows you to efficiently fit multiple `StatsForecast` models
    for large sets of time series. It operates with pandas DataFrame `df` that identifies series
    and datestamps with the `unique_id` and `ds` columns. The `y` column denotes the target
    time series variable.

    The class has memory-efficient `StatsForecast.forecast` method that avoids storing partial
    model outputs. While the `StatsForecast.fit` and `StatsForecast.predict` methods with
    Scikit-learn interface store the fitted models.

    The `StatsForecast` class offers parallelization utilities with Dask, Spark and Ray back-ends.
    See distributed computing example [here](https://github.com/Nixtla/statsforecast/tree/main/experiments/ray).

    Parameters
    ----------
    models : List[Any]
        List of instantiated objects models.StatsForecast.
    freq : str
        Frequency of the data.
        See [panda's available frequencies](https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases).
    n_jobs : int (default=1)
        Number of jobs used in the parallel processing, use -1 for all cores.
    df : pandas.DataFrame | pl.DataFrame, optional (default=None)
        DataFrame with columns [`unique_id`, `ds`, `y`] and exogenous.
    sort_df : bool (default=True)
        If True, sort `df` by [`unique_id`,`ds`].
    fallback_model : Any, optional (default=None)
        Model to be used if a model fails.
        Only works with the `forecast` and `cross_validation` methods.
    verbose : bool (default=True)
        Prints TQDM progress bar when `n_jobs=1`.
    """

    def forecast(
        self,
        h: int,
        df: Any = None,
        X_df: Optional[Union[pd.DataFrame, pl.DataFrame]] = None,
        level: Optional[List[int]] = None,
        fitted: bool = False,
        sort_df: bool = True,
        prediction_intervals: Optional[ConformalIntervals] = None,
    ):
        if prediction_intervals is not None and level is None:
            raise ValueError(
                "You must specify `level` when using `prediction_intervals`"
            )
        if self._is_native(df=df):
            return super().forecast(
                h=h,
                df=df,
                X_df=X_df,
                level=level,
                fitted=fitted,
                sort_df=sort_df,
                prediction_intervals=prediction_intervals,
            )
        assert df is not None
        engine = make_execution_engine(infer_by=[df])
        backend = make_backend(engine)
        return backend.forecast(
            df=df,
            models=self.models,
            freq=self.freq,
            fallback_model=self.fallback_model,
            h=h,
            X_df=X_df,
            level=level,
            fitted=fitted,
            prediction_intervals=prediction_intervals,
        )

    def cross_validation(
        self,
        h: int,
        df: Any = None,
        n_windows: int = 1,
        step_size: int = 1,
        test_size: Optional[int] = None,
        input_size: Optional[int] = None,
        level: Optional[List[int]] = None,
        fitted: bool = False,
        refit: bool = True,
        sort_df: bool = True,
        prediction_intervals: Optional[ConformalIntervals] = None,
    ):
        if self._is_native(df=df):
            return super().cross_validation(
                h=h,
                df=df,
                n_windows=n_windows,
                step_size=step_size,
                test_size=test_size,
                input_size=input_size,
                level=level,
                fitted=fitted,
                refit=refit,
                sort_df=sort_df,
                prediction_intervals=prediction_intervals,
            )
        assert df is not None
        engine = make_execution_engine(infer_by=[df])
        backend = make_backend(engine)
        return backend.cross_validation(
            df=df,
            models=self.models,
            freq=self.freq,
            fallback_model=self.fallback_model,
            h=h,
            n_windows=n_windows,
            step_size=step_size,
            test_size=test_size,
            input_size=input_size,
            level=level,
            refit=refit,
            fitted=fitted,
            prediction_intervals=prediction_intervals,
        )

    def _is_native(self, df) -> bool:
        engine = try_get_context_execution_engine()
        return engine is None and (
            df is None or isinstance(df, pd.DataFrame) or isinstance(df, pl.DataFrame)
        )
